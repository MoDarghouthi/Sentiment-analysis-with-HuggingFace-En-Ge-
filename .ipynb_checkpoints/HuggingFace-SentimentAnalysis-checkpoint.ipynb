{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80565264-7e94-46af-94f8-f080a17d6441",
   "metadata": {},
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af011b0c-c0f8-4b41-99ad-a9260d242acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import torch\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "220e81e7-2c75-4fcf-98da-6374d324bc16",
   "metadata": {},
   "source": [
    "# Using Hugging face's ready-to-use pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62da06ce-52d3-44bf-8fab-66220289cd21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6a905b778de4c11833952aa2a47d843",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/629 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe53166974024226ad0e7191a0d5b3f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/255M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05aff8f89100417fa77f01c14c40e9ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e78eabb0ab4543b196c5b1e2f733727a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/226k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9998455047607422},\n",
       " {'label': 'NEGATIVE', 'score': 0.9907469749450684}]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = pipeline('sentiment-analysis')\n",
    "result = classifier([\"Darghouthi is an awesome developer\",\n",
    "                     \"I hate the fact that I love you\"])\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18189721-27b4-4cdc-81df-455f8ec07e0a",
   "metadata": {},
   "source": [
    "# Using a BERT transformer fine-tuned for english"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ad88e0c-cee0-4497-b8d6-dbbe4858770c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9998455047607422},\n",
       " {'label': 'NEGATIVE', 'score': 0.9907469749450684}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "bertClassifier = pipeline(\"sentiment-analysis\", model=model_name)\n",
    "results = bertClassifier([\"Darghouthi is an awesome developer\",\n",
    "                          \"I hate the fact that I love you\"])\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e26594-5157-44f8-8fe1-7a21d1c061fb",
   "metadata": {},
   "source": [
    "# Adjusting the tokenizer and Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "518f35c0-3b35-4ddc-a05b-7a84633b5f1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9998455047607422},\n",
       " {'label': 'NEGATIVE', 'score': 0.9907469749450684}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "bertAutoClassifier = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "autoClassifier = pipeline(\"sentiment-analysis\", model = bertAutoClassifier, tokenizer= tokenizer)\n",
    "results = autoClassifier([\"Darghouthi is an awesome developer\",\n",
    "                          \"I hate the fact that I love you\"])\n",
    "results                                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e09f944a-553c-4ba9-bd24-8a3fab20641b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens: ['dar', '##gh', '##outh', '##i', 'is', 'an', 'awesome', 'developer']\n",
      "token_ids: [18243, 5603, 17167, 2072, 2003, 2019, 12476, 9722]\n",
      "input_ids: {'input_ids': [101, 18243, 5603, 17167, 2072, 2003, 2019, 12476, 9722, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer.tokenize(\"Darghouthi is an awesome developer\")\n",
    "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "input_ids = tokenizer(\"Darghouthi is an awesome developer\")\n",
    "print(f\"tokens: {tokens}\")\n",
    "print(f\"token_ids: {token_ids}\")\n",
    "print(f\"input_ids: {input_ids}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f905f4-c016-4ab5-9268-020d7d45bb91",
   "metadata": {},
   "source": [
    "# Fine tuning the Classifier on a custom Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d04ef955-0f0a-4bfd-9f46-fcc2eaabecd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SequenceClassifierOutput(loss=None, logits=tensor([[-4.2225,  4.5533],\n",
      "        [ 2.5521, -2.1214],\n",
      "        [-1.9875,  2.0248]]), hidden_states=None, attentions=None)\n",
      "tensor([[1.5441e-04, 9.9985e-01],\n",
      "        [9.9075e-01, 9.2530e-03],\n",
      "        [1.7770e-02, 9.8223e-01]])\n",
      "tensor([1, 0, 1])\n",
      "['POSITIVE', 'NEGATIVE', 'POSITIVE']\n"
     ]
    }
   ],
   "source": [
    "X_train = [\"Darghouthi is an awesome developer\",\n",
    "           \"I hate the fact that I love you\",\n",
    "           \"the food here is a delicacy\"]\n",
    "batch = tokenizer(X_train, padding= True, truncation=True, max_length=256, return_tensors=\"pt\")\n",
    "with torch.no_grad():\n",
    "    output = bertAutoClassifier(**batch)\n",
    "    print(output)\n",
    "    predictions = F.softmax(output.logits, dim=1)\n",
    "    print(predictions)\n",
    "    labels = torch.argmax(predictions, dim=1)\n",
    "    print(labels)\n",
    "    labels = [bertAutoClassifier.config.id2label[label_id] for label_id in labels.tolist()]\n",
    "    print(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4550fc-861f-4281-93e5-397791ff623a",
   "metadata": {},
   "source": [
    "# Sentiment classification on German sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cd2456c8-ddd5-453a-99bd-a93da1b220bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5098dee2d3f14e5e97ff47273c12c363",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/161 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e5a4e4f8fdf4e42964426d7eda95b3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebec05287ff247d4885ccf765e4b47b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/249k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7796d92ed6744135a87106f17c3b026c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45df003f2b11463abe7e9b83c9bc91b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/416M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 3.5039, -0.4604, -3.4561],\n",
      "        [ 0.2645,  3.1876, -5.1879],\n",
      "        [ 0.1335, -0.8306,  1.3824]]), hidden_states=None, attentions=None)\n",
      "tensor([[9.8046e-01, 1.8611e-02, 9.3052e-04],\n",
      "        [5.1010e-02, 9.4877e-01, 2.1864e-04],\n",
      "        [2.0543e-01, 7.8338e-02, 7.1623e-01]])\n",
      "tensor([0, 1, 2])\n",
      "['positive', 'negative', 'neutral']\n"
     ]
    }
   ],
   "source": [
    "geman_model_name = \"oliverguhr/german-sentiment-bert\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(geman_model_name)\n",
    "german_model = AutoModelForSequenceClassification.from_pretrained(geman_model_name)\n",
    "\n",
    "german_sentences = [\"Passau ist eine sehr schöne Stadt.\", \"die Pizza schmeckt nicht gut\", \"Deutschland hat die perfekte Work-Life-Balance\"]\n",
    "german_batch = tokenizer(german_sentences, padding= True, truncation=True, max_length=256, return_tensors=\"pt\")\n",
    "with torch.no_grad():\n",
    "    output = german_model(**german_batch)\n",
    "    print(output)\n",
    "    predictions = F.softmax(output.logits, dim=1)\n",
    "    print(predictions)\n",
    "    labels = torch.argmax(predictions, dim=1)\n",
    "    print(labels)\n",
    "    labels = [german_model.config.id2label[label_id] for label_id in labels.tolist()]\n",
    "    print(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c892fa-35ac-423b-93ec-90a281b26d78",
   "metadata": {},
   "source": [
    "# IMDB reviews Sentiment analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f91d0bfd-157e-4b47-984f-bee37e22ce49",
   "metadata": {},
   "source": [
    "## 1- Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2f7b4bc6-fa4e-4656-bee6-775ee4296d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import DataCollatorWithPadding\n",
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461f162a-6afe-4005-aca4-bad41c50b3ae",
   "metadata": {},
   "source": [
    "## 2- Data pre-processsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "355dee0a-6365-49f0-8b8d-c838df9200de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "465a1eb0394c407991a33df99fd262b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/1.79k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c1b06a28389464a8c7169e03c5cc986",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading metadata:   0%|          | 0.00/1.05k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset imdb/plain_text (download: 80.23 MiB, generated: 127.02 MiB, post-processed: Unknown size, total: 207.25 MiB) to /home/darghouthi/.cache/huggingface/datasets/imdb/plain_text/1.0.0/2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd06465c9a1745b384bfc9332357725d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/84.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating unsupervised split:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset imdb downloaded and prepared to /home/darghouthi/.cache/huggingface/datasets/imdb/plain_text/1.0.0/2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2758deb6d8e74dfca049da1aff7bc07d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce7e61f68f2b42e493611ff008089f77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee697958ace14bdfbdb2f94d07ed174c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/483 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82e37a51ff564d4faad7b45c0fc1de75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/226k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5d72b68ea7d452cb4f820262432132b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/455k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function preprocess_function at 0x7f461f79f160> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe3e3e236c9740d39fac5d2caf5e656d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af4af8126e7c4e849c7a80c2a0655f28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a77ab3ca18e496ca7910c29cc839034",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "imdb = load_dataset(\"imdb\")\n",
    "imdb[\"test\"][0]\n",
    "\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True)\n",
    "\n",
    "tokenized_imdb = imdb.map(preprocess_function, batched=True)\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"tf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2910a19b-83d9-4a6e-b8e6-6728378ab503",
   "metadata": {},
   "source": [
    "## 3- Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c0574721-afa0-4898-88d4-1e82b180673c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertForSequenceClassification: ['vocab_projector', 'vocab_layer_norm', 'vocab_transform', 'activation_13']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier', 'dropout_119', 'classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour, please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "25000/25000 [==============================] - 4279s 171ms/step - loss: 0.2713 - val_loss: 0.1988\n",
      "Epoch 2/2\n",
      "25000/25000 [==============================] - 4325s 173ms/step - loss: 0.1089 - val_loss: 0.1914\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f456c04b820>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_train_set = tokenized_imdb[\"train\"].to_tf_dataset(\n",
    "    columns=[\"attention_mask\", \"input_ids\", \"label\"],\n",
    "    shuffle=True,\n",
    "    batch_size=1,\n",
    "    collate_fn=data_collator,\n",
    ")\n",
    "\n",
    "tf_validation_set = tokenized_imdb[\"test\"].to_tf_dataset(\n",
    "    columns=[\"attention_mask\", \"input_ids\", \"label\"],\n",
    "    shuffle=False,\n",
    "    batch_size=1,\n",
    "    collate_fn=data_collator,\n",
    ")\n",
    "from transformers import create_optimizer\n",
    "import tensorflow as tf\n",
    "\n",
    "batch_size = 1\n",
    "num_epochs = 2\n",
    "batches_per_epoch = len(tokenized_imdb[\"train\"]) // batch_size\n",
    "total_train_steps = int(batches_per_epoch * num_epochs)\n",
    "optimizer, schedule = create_optimizer(init_lr=2e-5, num_warmup_steps=0, num_train_steps=total_train_steps)\n",
    "from transformers import TFAutoModelForSequenceClassification\n",
    "\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=2)\n",
    "model.compile(optimizer=optimizer)\n",
    "model.fit(x=tf_train_set, validation_data=tf_validation_set, epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2ea6a6bd-0709-4546-9c17-1c5c145a8dc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - 1084s 43ms/step\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(tf_validation_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "394a4f1b-1d3f-4f98-bae8-4c5929981ac3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TFSequenceClassifierOutput(loss=array([0.00117674, 0.01438309, 0.0067858 , ..., 0.7950314 , 0.05493376,\n",
       "       0.01522068], dtype=float32), logits=array([[ 3.326938  , -3.4174528 ],\n",
       "       [ 2.1119304 , -2.12257   ],\n",
       "       [ 2.4903588 , -2.4991648 ],\n",
       "       ...,\n",
       "       [ 0.06064893, -0.13369226],\n",
       "       [-1.4517492 ,  1.4222863 ],\n",
       "       [-2.1142852 ,  2.0631912 ]], dtype=float32), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "74d0ddd0-8a95-40e1-9662-e1b165fc4f00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 1, 1])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "preds = np.argmax(predictions.logits, axis=-1)\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "aa175f9a-0f63-4af4-a6cd-649c9f81defc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.93368, 'f1': 0.933424349502088}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_metric\n",
    "\n",
    "metric = load_metric(\"glue\", \"mrpc\")\n",
    "metric.compute(predictions=preds, references=imdb[\"test\"][\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912d8f1d-d468-4be6-b1c8-95a371eb0361",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
